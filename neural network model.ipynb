{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0e0c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as math\n",
    "import random as rd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4bdd50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "(17398, 1199)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "# drop these columns (\"x96\", \"x97\", \"x98\", \"x1021\", \"x1098\", \"x1099\" are columns with no data)\n",
    "train_X = train_data.drop([\"health\", \"personid\", \"uniqueid\", \"year\", \"x96\", \"x97\", \"x98\", \"x1021\", \"x1098\", \"x1099\"], axis = 1)\n",
    "\n",
    "# change all numerical values to float type\n",
    "train_X = train_X.astype(float)\n",
    "\n",
    "# drop NA that accounts a certain percentage\n",
    "threshold = math.floor(0.5 * 17398)\n",
    "train_X_less_na = train_X.dropna(thresh=threshold, axis=1)\n",
    "print(len(train_X_less_na.columns))\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c85462ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "# find numerical variables and store them in columns_to_drop \n",
    "category_vars = []\n",
    "for c in train_X_less_na:\n",
    "    if len(pd.unique(train_X_less_na[c])) < 14:\n",
    "        category_vars.append(c)\n",
    "\n",
    "print(len(category_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa57dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute our data by using the mean of a column as the missing values\n",
    "\n",
    "train_X_less_na_imp = train_X_less_na.copy()\n",
    "\n",
    "for name in train_X_less_na_imp.columns:\n",
    "    if (name in category_vars):\n",
    "        mode = train_X_less_na_imp[name].mode()\n",
    "        for i in range(0, len(train_X_less_na_imp[name])):\n",
    "            if (np.isnan(train_X_less_na_imp.at[i, name])):\n",
    "                train_X_less_na_imp.at[i, name] = rd.choices(mode, k=1)[0]\n",
    "    else:\n",
    "        mean = train_X_less_na_imp[name].mean()\n",
    "        for i in range(0, len(train_X_less_na_imp[name])):\n",
    "            if (np.isnan(train_X_less_na_imp.at[i, name])):\n",
    "                train_X_less_na_imp.at[i, name] = mean\n",
    "\n",
    "\n",
    "# for c in train_X_less_na_imp:\n",
    "    #if c in category_vars:\n",
    "     #   train_X_less_na_imp[c].fillna(train_X_less_na_imp[c].mode(),inplace=True)\n",
    "    # else:\n",
    "     #   train_X_less_na_imp[c].fillna(train_X_less_na_imp[c].mean(),inplace=True)\n",
    "        \n",
    "# check if there is still any NAs\n",
    "for c in train_X_less_na_imp:\n",
    "    if train_X_less_na_imp[c].isnull().any():\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec7c102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530\n"
     ]
    }
   ],
   "source": [
    "# scale train_X_less_na_imp to fit One class SVM\n",
    "train_X_scaled = train_X_less_na_imp.copy()\n",
    "\n",
    "for c in train_X_scaled:\n",
    "    if c not in category_vars:\n",
    "        if (max(train_X_scaled[c]) > 1) or (min(train_X_scaled[c]) < 0):\n",
    "            train_X_scaled[c] = (train_X_scaled[c]-train_X_scaled[c].mean())/train_X_scaled[c].std()\n",
    "        \n",
    "train_X_scaled = pd.get_dummies(train_X_scaled, columns = category_vars, drop_first = True) # change categorical variables to indicator variables\n",
    "\n",
    "print(len(train_X_scaled.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1423752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers for outcomes in class = 1\n",
    "one_class_svm_1 = OneClassSVM(gamma='scale', nu=0.01)\n",
    "train_X_scaled_1 = train_X_scaled[train_data.health==1]\n",
    "one_class_svm_1.fit(train_X_scaled_1)\n",
    "outliers_1 = one_class_svm_1.predict(train_X_scaled_1) # outliers are indicated as -1, inliers are indicated as 1\n",
    "\n",
    "# outliers for outcomes in class = 2\n",
    "one_class_svm_2 = OneClassSVM(gamma='scale', nu=0.01)\n",
    "train_X_scaled_2 = train_X_scaled[train_data.health==2]\n",
    "one_class_svm_2.fit(train_X_scaled_2)\n",
    "outliers_2 = one_class_svm_2.predict(train_X_scaled_2) # outliers are indicated as -1, inliers are indicated as 1\n",
    "\n",
    "# outliers for outcomes in class = 3\n",
    "one_class_svm_3 = OneClassSVM(gamma='scale', nu=0.01)\n",
    "train_X_scaled_3 = train_X_scaled[train_data.health==3]\n",
    "one_class_svm_3.fit(train_X_scaled_3)\n",
    "outliers_3 = one_class_svm_3.predict(train_X_scaled_3) # outliers are indicated as -1, inliers are indicated as 1\n",
    "\n",
    "# outliers for outcomes in class = 4\n",
    "one_class_svm_4 = OneClassSVM(gamma='scale', nu=0.01)\n",
    "train_X_scaled_4 = train_X_scaled[train_data.health==4]\n",
    "one_class_svm_4.fit(train_X_scaled_4)\n",
    "outliers_4 = one_class_svm_4.predict(train_X_scaled_4) # outliers are indicated as -1, inliers are indicated as 1  \n",
    "\n",
    "# outliers for outcomes in class = 5\n",
    "one_class_svm_5 = OneClassSVM(gamma='scale', nu=0.01)\n",
    "train_X_scaled_5 = train_X_scaled[train_data.health==5]\n",
    "one_class_svm_5.fit(train_X_scaled_5)\n",
    "outliers_5 = one_class_svm_5.predict(train_X_scaled_5) # outliers are indicated as -1, inliers are indicated as 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "375c3de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17093, 179)\n"
     ]
    }
   ],
   "source": [
    "# remove outliers from train_X_less_na_imp\n",
    "outliers = [0 for i in range(0, len(train_data.health))]\n",
    "\n",
    "i_1 = 0\n",
    "i_2 = 0\n",
    "i_3 = 0\n",
    "i_4 = 0\n",
    "i_5 = 0\n",
    "cls_1 = (train_data.health==1)\n",
    "cls_2 = (train_data.health==2)\n",
    "cls_3 = (train_data.health==3)\n",
    "cls_4 = (train_data.health==4)\n",
    "cls_5 = (train_data.health==5)\n",
    "for j in range(0, len(train_data.health)):\n",
    "    if cls_1[j] == True:\n",
    "        outliers[j] = outliers_1[i_1]\n",
    "        i_1 += 1\n",
    "    elif cls_2[j] == True:\n",
    "        outliers[j] = outliers_2[i_2]\n",
    "        i_2 += 1\n",
    "    elif cls_3[j] == True:\n",
    "        outliers[j] = outliers_3[i_3]\n",
    "        i_3 += 1\n",
    "    elif cls_4[j] == True:\n",
    "        outliers[j] = outliers_4[i_4]\n",
    "        i_4 += 1\n",
    "    elif cls_5[j] == True:\n",
    "        outliers[j] = outliers_5[i_5]\n",
    "        i_5 += 1 \n",
    "train_X_scaled['outliers'] = outliers\n",
    "train_X_less_na_imp = train_X_less_na_imp[train_X_scaled.outliers == 1].reset_index(drop = True)\n",
    "train_no_outlier = train_data[train_X_scaled.outliers == 1].reset_index(drop = True)\n",
    "train_X_no_outlier = train_X[train_X_scaled.outliers == 1].reset_index(drop = True)\n",
    "print(train_X_less_na_imp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dba500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17093, 170)\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "# Removing highly correlated features\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = train_X_less_na_imp.corr(method='pearson').abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop features \n",
    "train_X_less_na_imp_corr = train_X_less_na_imp.drop(to_drop, axis=1)\n",
    "print(train_X_less_na_imp_corr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20fd6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered variables based on their importance derived from RF models\n",
    "selector_rf = RFE(RandomForestClassifier(n_estimators=10, oob_score=True, criterion=\"entropy\", min_samples_split = 5), n_features_to_select=200, step=2)\n",
    "# selector_rf = RFECV(RandomForestClassifier(n_estimators=10, criterion=\"entropy\", min_samples_split = 5), step = 2, cv = 5, scoring= \"neg_log_loss\")\n",
    "selector_rf = selector_rf.fit(train_X_less_na_imp_corr, train_no_outlier.health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67698281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "x4\n",
      "x7\n",
      "x14\n",
      "x17\n",
      "x19\n",
      "x162\n",
      "x163\n",
      "x164\n",
      "x195\n",
      "x223\n",
      "x224\n",
      "x225\n",
      "x226\n",
      "x227\n",
      "x228\n",
      "x229\n",
      "x230\n",
      "x452\n",
      "x472\n",
      "x477\n",
      "x544\n",
      "x545\n",
      "x546\n",
      "x547\n",
      "x548\n",
      "x595\n",
      "x596\n",
      "x597\n",
      "x613\n",
      "x614\n",
      "x615\n",
      "x616\n",
      "x617\n",
      "x630\n",
      "x631\n",
      "x632\n",
      "x634\n",
      "x635\n",
      "x638\n",
      "x639\n",
      "x640\n",
      "x641\n",
      "x642\n",
      "x643\n",
      "x644\n",
      "x645\n",
      "x646\n",
      "x647\n",
      "x648\n",
      "x649\n",
      "x650\n",
      "x651\n",
      "x652\n",
      "x655\n",
      "x657\n",
      "x659\n",
      "x681\n",
      "x715\n",
      "x718\n",
      "x723\n",
      "x725\n",
      "x726\n",
      "x728\n",
      "x729\n",
      "x730\n",
      "x754\n",
      "x758\n",
      "x759\n",
      "x766\n",
      "x767\n",
      "x768\n",
      "x769\n",
      "x770\n",
      "x771\n",
      "x772\n",
      "x773\n",
      "x774\n",
      "x775\n",
      "x776\n",
      "x777\n",
      "x778\n",
      "x779\n",
      "x782\n",
      "x784\n",
      "x893\n",
      "x896\n",
      "x897\n",
      "x898\n",
      "x901\n",
      "x902\n",
      "x906\n",
      "x907\n",
      "x908\n",
      "x909\n",
      "x910\n",
      "x911\n",
      "x912\n",
      "x920\n",
      "x921\n",
      "x923\n",
      "x929\n",
      "x930\n",
      "x931\n",
      "x934\n",
      "x935\n",
      "x939\n",
      "x940\n",
      "x941\n",
      "x942\n",
      "x943\n",
      "x944\n",
      "x945\n",
      "x953\n",
      "x954\n",
      "x956\n",
      "x961\n",
      "x963\n",
      "x964\n",
      "x965\n",
      "x966\n",
      "x967\n",
      "x968\n",
      "x970\n",
      "x1032\n",
      "x1033\n",
      "x1035\n",
      "x1036\n",
      "x1039\n",
      "x1106\n",
      "x1108\n",
      "x1110\n",
      "x1132\n",
      "x1133\n",
      "x1140\n",
      "x1141\n",
      "x1143\n",
      "x1144\n",
      "x1145\n",
      "x1146\n",
      "x1147\n",
      "x1150\n",
      "x1152\n",
      "x1153\n",
      "x1157\n",
      "x1158\n",
      "x1159\n",
      "x1160\n",
      "x1161\n",
      "x1162\n",
      "x1164\n",
      "x1165\n",
      "x1166\n",
      "x1167\n",
      "x1175\n",
      "x1176\n",
      "x1177\n",
      "x1178\n",
      "x1179\n",
      "x1181\n",
      "x1182\n",
      "x1183\n",
      "x1184\n",
      "x1185\n",
      "x1201\n",
      "x1202\n",
      "x1203\n",
      "x1204\n"
     ]
    }
   ],
   "source": [
    "# the selected features\n",
    "features_rf = train_X_less_na_imp_corr.columns[selector_rf.get_support()]\n",
    "print(len(features_rf))\n",
    "for f in features_rf:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2b8a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# convert the test dataset to float type\n",
    "test_data = test_data.astype(float)\n",
    "\n",
    "# features_rf = features_rf.insert(0, \"personid\")\n",
    "test_data_clean = test_data[features_rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edfb0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute test data\n",
    "\n",
    "test_data_clean_imp = test_data_clean.copy()\n",
    "\n",
    "for name in test_data_clean_imp.columns:\n",
    "    if (name in category_vars):\n",
    "        mode = test_data_clean_imp[name].mode()\n",
    "        for i in range(0, len(test_data_clean_imp[name])):\n",
    "            if (np.isnan(test_data_clean_imp.at[i, name])):\n",
    "                test_data_clean_imp.at[i, name] = rd.choices(mode, k=1)[0]\n",
    "    else:\n",
    "        mean = test_data_clean_imp[name].mean()\n",
    "        for i in range(0, len(test_data_clean_imp[name])):\n",
    "            if (np.isnan(test_data_clean_imp.at[i, name])):\n",
    "                test_data_clean_imp.at[i, name] = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fd2e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-impute train X data after removing outliers\n",
    "train_X_no_outlier_final = train_X_no_outlier[features_rf]\n",
    "\n",
    "for name in train_X_no_outlier_final.columns:\n",
    "    if (name in category_vars):\n",
    "        mode = train_X_no_outlier_final[name].mode()\n",
    "        for i in range(0, len(train_X_no_outlier_final[name])):\n",
    "            if (np.isnan(train_X_no_outlier_final.at[i, name])):\n",
    "                train_X_no_outlier_final.at[i, name] = rd.choices(mode, k=1)[0]\n",
    "    else:\n",
    "        mean = train_X_no_outlier_final[name].mean()\n",
    "        for i in range(0, len(train_X_no_outlier_final[name])):\n",
    "            if (np.isnan(train_X_no_outlier_final.at[i, name])):\n",
    "                train_X_no_outlier_final.at[i, name] = mean\n",
    "\n",
    "# check if there is still any NAs\n",
    "for c in train_X_no_outlier_final:\n",
    "    if train_X_no_outlier_final[c].isnull().any():\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "037b370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale continuous data\n",
    "\n",
    "for c in train_X_no_outlier_final.columns:\n",
    "    if c not in category_vars:\n",
    "        if (max(train_X_no_outlier_final[c]) > 1) or (min(train_X_no_outlier_final[c]) < 0):\n",
    "            train_X_no_outlier_final[c] = (train_X_no_outlier_final[c]-train_X_no_outlier_final[c].mean())/train_X_no_outlier_final[c].std()\n",
    "\n",
    "for c in test_data_clean_imp.columns:\n",
    "    if c not in category_vars:\n",
    "        if (max(test_data_clean_imp[c]) > 1) or (min(test_data_clean_imp[c]) < 0):\n",
    "            test_data_clean_imp[c] = (test_data_clean_imp[c]-test_data_clean_imp[c].mean())/test_data_clean_imp[c].std()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a42842b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert X dataset's categorical variables to dummies\n",
    "train_X_log_cat_columns = []\n",
    "for c in train_X_no_outlier_final.columns:\n",
    "    if c in category_vars:\n",
    "        train_X_log_cat_columns.append(c)\n",
    "\n",
    "train_X_no_outlier_final = pd.get_dummies(train_X_no_outlier_final, columns = train_X_log_cat_columns, drop_first = True)\n",
    "\n",
    "# convert test dataset's categorical variables to dummies\n",
    "test_log_cat_columns = []\n",
    "for c in test_data_clean_imp.columns:\n",
    "    if c in category_vars:\n",
    "        test_log_cat_columns.append(c)\n",
    "\n",
    "test_data_clean_imp = pd.get_dummies(test_data_clean_imp, columns = test_log_cat_columns, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c40d099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the mismatching dummy X values between test and train\n",
    "\n",
    "for names in test_data_clean_imp.columns.tolist():\n",
    "    if (names not in train_X_no_outlier_final.columns.tolist()):\n",
    "        test_data_clean_imp = test_data_clean_imp.drop(names, axis = 1)\n",
    "        \n",
    "for names in train_X_no_outlier_final.columns.tolist():\n",
    "    if (names not in test_data_clean_imp.columns.tolist()):\n",
    "        test_data_clean_imp[names] = [0]* len(test_data_clean_imp)\n",
    "\n",
    "## correct order\n",
    "test_data_clean_imp = test_data_clean_imp[train_X_no_outlier_final.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ede21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of parameters to perform Cross-validation\n",
    "parameters = {'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
    "              'batch_size': [32, 64, 128, 256, 500],\n",
    "              'hidden_layer_sizes': [[9,9,], [20,10],[47,47,],[87,47,], [9,5,], [20,],\n",
    "                                     [9,],[87,],[100,]]}\n",
    "\n",
    "nn_2 = MLPClassifier(max_iter = 500, learning_rate_init = 0.1, learning_rate = \"adaptive\") \n",
    "clf_nn_2 = GridSearchCV(nn_2, param_grid = parameters, scoring = \"neg_log_loss\") \n",
    "nn_model_2 = clf_nn_2.fit(X2, Y) # the final Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d143091",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn_model_2.best_score_)\n",
    "nn_model_2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the neural network model's prediction\n",
    "predict_nn = nn_model_2.predict_proba(test_data_clean_imp)\n",
    "predict_nn = pd.DataFrame(predict_nn, columns = [\"p1\", \"p2\", \"p3\", \"p4\", \"p5\"])\n",
    "predict_nn.insert(loc = 0, column = 'uniqueid', value = test_data.uniqueid.astype(int))\n",
    "#predict_nn.to_csv(r'C:\\Users\\AlecZZX\\Desktop\\441 data\\prediction trials\\prediction_10.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8d300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ac364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab6e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
